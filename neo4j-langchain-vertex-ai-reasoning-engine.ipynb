{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "vertexAIreasoning.ipynb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GraphRAG with Neo4j and LangChain and Gemini on VertexAI Reasoning Engine\n",
        "\n",
        "This is a demonstration of a GeNAI API with advanced RAG patterns combining vector and graph search.\n",
        "\n",
        "It is deployed on Vertex AI Reasoning Engine (Preview) as scalable infrastructure and can then be integrated with GenAI applications on Cloud Run via REST APIs.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "The dataset is a graph about companies, associated industries, and people and articles that report on those companies.\n",
        "\n",
        "![Graph Model](https://i.imgur.com/lWJZSEe.png)\n",
        "\n",
        "The articles are chunked and the chunks also stored in the graph.\n",
        "\n",
        "Embeddings are computed for each of the text chunks with `textembedding-gecko` (786 dim) and stored on each chunk node.\n",
        "A Neo4j vector index `news_google` and a fulltext index `news_fulltext` (for hybrid search) were created.\n",
        "\n",
        "The database is publicly available with a readonly user:\n",
        "\n",
        "https://demo.neo4jlabs.com:7473/browser/\n",
        "\n",
        "* URI: neo4j+s://demo.neo4jlabs.com\n",
        "* User: companies\n",
        "* Password: companies\n",
        "* Companies: companies\n",
        "\n",
        "We utilize the Neo4jVector LangChain integration, which allows for advanced RAG patterns.\n",
        "We will utilize both hybrid search as well as parent-child retrievers and GraphRAG (extract relevant context).\n",
        "\n",
        "In our configuration we provide both the vector and fulltext index as well as a retrieval query that fetches the following additional information for each chunk\n",
        "\n",
        "* Parent `Article` of the `Chunk` (aggregate all chunks for a single article)\n",
        "* `Organization`(s) mentioned\n",
        "* `IndustryCategory`(ies) for the organization\n",
        "* `Person`(s) connected to the organization and their roles (e.g. investor, chairman, ceo)\n",
        "\n",
        "We will retrieve the top-k = 5 results from the vector index.\n",
        "\n",
        "As LLM we will utilize Vertex AI *Gemini Pro 1.0*\n",
        "\n",
        "We use a temperature of 0.1, top-k=40, top-p=0.8\n",
        "\n",
        "Our `LangchainCode` class contains the methods for initialization which can only hold serializable information (strings and numbers).\n",
        "\n",
        "In `set_up()` Gemini as LLM, VertexAI Embeddings and the `Neo4jVector` retriever are combined into a LangChain chain.\n",
        "\n",
        "Which is then used in `query`  with `chain.invoke()`.\n",
        "\n",
        "The class is deployed as ReasoningEngine with the Google Vertex AI Python SDK.\n",
        "For the deployment you provide the instance of the class which captures relevant environment variables and configuration and the dependencies, in our case `google-cloud-vertexai, langchain, langchain_google_vertexai, neo4j`.\n",
        "\n",
        "And after successful deploymnet we can use the resulting object via the `query` method, passing in our user question."
      ],
      "metadata": {
        "id": "6u4maG9UQ685"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"vertex-ai-neo4j-extension\"\n",
        "REGION = \"us-central1\"\n",
        "STAGING_BUCKET = \"gs://neo4j-vertex-ai-extension\"\n"
      ],
      "metadata": {
        "id": "3mIXWnf_7Vtz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716387442760,
          "user_tz": -120,
          "elapsed": 118,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user(project_id=PROJECT_ID)\n",
        "\n",
        "!gcloud config set project vertex-ai-neo4j-extension"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DW7SGmhwRyoJ",
        "outputId": "5def019d-fa0d-4790-bcb4-de3df28908f5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716387446921,
          "user_tz": -120,
          "elapsed": 2825,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet neo4j==5.19.0\n",
        "!pip install --quiet langchain_google_vertexai==1.0.4\n",
        "!pip install --quiet --force-reinstall langchain==0.2.0 langchain_community==0.2.0\n"
      ],
      "metadata": {
        "id": "wxp09CthdF89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716387394430,
          "user_tz": -120,
          "elapsed": 70436,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "a3df57c2-4abc-4606-e2db-d7ba87559aa6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet google-cloud-aiplatform==1.51.0 # google-cloud-aiplatform[reasoningengine,langchain]\n",
        "\n",
        "# !gsutil cp gs://vertex_sdk_private_releases/llm_extension/google_cloud_aiplatform-1.45.dev20240328+llm.extension-py2.py3-none-any.whl .\n",
        "\n",
        "# !pip install --quiet bigframes==0.26.0\n",
        "\n",
        "# !pip install --force-reinstall --quiet google_cloud_aiplatform-1.45.dev20240328+llm.extension-py2.py3-none-any.whl[extension]\n",
        "# This is for printing the Vertex AI service account.\n",
        "# !pip install --upgrade --quiet google-cloud-resource-manager\n",
        "!pip install --quiet  google-cloud-resource-manager==1.12.3\n"
      ],
      "metadata": {
        "id": "9rpCl9j4Hpmh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716386739685,
          "user_tz": -120,
          "elapsed": 19630,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import bigframes.dataframe"
      ],
      "metadata": {
        "id": "EJR9LQk_50tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bugfix\n",
        "# import bigframes\n",
        "# import bigframes.pandas as bpd\n",
        "\n",
        "# bigframes.dataframe.DataFrame=bigframes.pandas.DataFrame"
      ],
      "metadata": {
        "id": "Aam0Cfxq6XJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "# from google.cloud.aiplatform.preview import llm_extension\n",
        "from vertexai.preview import reasoning_engines\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    staging_bucket=STAGING_BUCKET,\n",
        ")"
      ],
      "metadata": {
        "id": "guEu_Yh6Rv8n",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716387186458,
          "user_tz": -120,
          "elapsed": 2992,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\n",
        "from langchain_community.vectorstores import Neo4jVector\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n"
      ],
      "metadata": {
        "id": "mgNDGcXF-7zV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716387454554,
          "user_tz": -120,
          "elapsed": 3839,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you installed any packages above, you can restart the runtime to pick them up:\n",
        "\n",
        "Click on the \"Runtime\" button at the top of Colab.\n",
        "Select \"Restart session\".\n",
        "You would not need to re-run the cell above this one (to reinstall the packages)."
      ],
      "metadata": {
        "id": "HEljoz48bw6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "URI = os.getenv('NEO4J_URI', 'neo4j+s://demo.neo4jlabs.com')\n",
        "USER = os.getenv('NEO4J_USERNAME','companies')\n",
        "PASSWORD = os.getenv('NEO4J_PASSWORD','companies')\n",
        "DATABASE = os.getenv('NEO4J_DATABASE','companies')\n",
        "\n",
        "class LangchainCode:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"gemini-1.5-pro-preview-0409\" #\"gemini-pro\"\n",
        "        self.max_output_tokens = 1024\n",
        "        self.temperature = 0.1\n",
        "        self.top_p = 0.8\n",
        "        self.top_k = 40\n",
        "        self.project_id = PROJECT_ID\n",
        "        self.location = REGION\n",
        "        self.uri = URI\n",
        "        self.username = USER\n",
        "        self.password = PASSWORD\n",
        "        self.database = DATABASE\n",
        "        self.prompt_input_variables = [\"query\"]\n",
        "        self.prompt_template=\"\"\"\n",
        "            You are a venture capital assistant that provides useful answers about companies, their boards, financing etc.\n",
        "            only using the information from a company database already provided in the context.\n",
        "            Prefer higher rated information in your context and add source links in your answers.\n",
        "            Context: {context}\"\"\"\n",
        "\n",
        "def configure_qa_rag_chain(self, llm, embeddings):\n",
        "    qa_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(self.prompt_template),\n",
        "        HumanMessagePromptTemplate.from_template(\"Question: {question}\"\n",
        "                                                  \"\\nWhat else can you tell me about it?\"),\n",
        "    ])\n",
        "\n",
        "    # Vector + Knowledge Graph response\n",
        "    kg = Neo4jVector.from_existing_index(\n",
        "        embedding=embeddings,\n",
        "        url=self.uri, username=self.username, password=self.password,database=self.database,\n",
        "        search_type=\"hybrid\",\n",
        "        keyword_index_name=\"news_fulltext\",\n",
        "        index_name=\"news_google\",\n",
        "        retrieval_query=\"\"\"\n",
        "          WITH node as c,score\n",
        "          MATCH (c)<-[:HAS_CHUNK]-(article:Article)\n",
        "\n",
        "          WITH article, collect(distinct c.text) as texts, avg(score) as score\n",
        "          RETURN article {.title, .sentiment, .siteName, .summary,\n",
        "                organizations: [ (article)-[:MENTIONS]->(org:Organization) |\n",
        "                      org { .name, .revenue, .nbrEmployees, .isPublic, .motto, .summary,\n",
        "                      orgCategories: [ (org)-[:HAS_CATEGORY]->(i) | i.name],\n",
        "                      people: [ (org)-[rel]->(p:Person) | p { .name, .summary, role: replace(type(rel),\"HAS_\",\"\") }]}],\n",
        "                texts: texts} as text,\n",
        "          score, {source: article.siteName} as metadata\n",
        "        \"\"\",\n",
        "    )\n",
        "    retriever = kg.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    def format_docs(docs):\n",
        "      return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    chain = (\n",
        "        {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
        "#            RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "        | qa_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "    def set_up(self):\n",
        "        from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "        from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "        from langchain_community.vectorstores import Neo4jVector\n",
        "        from langchain_core.output_parsers import StrOutputParser\n",
        "        from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "        llm = ChatVertexAI(\n",
        "            model_name=self.model_name,\n",
        "            max_output_tokens=self.max_output_tokens,\n",
        "            max_input_tokens=32000,\n",
        "            temperature=self.temperature,\n",
        "            top_p=self.top_p,\n",
        "            top_k=self.top_k,\n",
        "            project = self.project_id,\n",
        "            location = self.location,\n",
        "            # convert_system_message_to_human=True,\n",
        "            response_validation=False,\n",
        "            verbose=True\n",
        "        )\n",
        "        embeddings = VertexAIEmbeddings(\"textembedding-gecko@001\")\n",
        "\n",
        "        self.qa_chain = self.configure_qa_rag_chain(llm, embeddings)\n",
        "\n",
        "    def query(self, query):\n",
        "        return self.qa_chain.invoke(query)"
      ],
      "metadata": {
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716387502985,
          "user_tz": -120,
          "elapsed": 160,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "id": "Xbq3bR_Knf8k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "URI = os.getenv('NEO4J_URI', 'neo4j+s://demo.neo4jlabs.com')\n",
        "USER = os.getenv('NEO4J_USERNAME','companies')\n",
        "PASSWORD = os.getenv('NEO4J_PASSWORD','companies')\n",
        "DATABASE = os.getenv('NEO4J_DATABASE','companies')\n",
        "\n",
        "class LangchainCode:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"gemini-1.5-pro-preview-0409\" #\"gemini-pro\"\n",
        "        self.max_output_tokens = 1024\n",
        "        self.temperature = 0.1\n",
        "        self.top_p = 0.8\n",
        "        self.top_k = 40\n",
        "        self.project_id = PROJECT_ID\n",
        "        self.location = REGION\n",
        "        self.uri = URI\n",
        "        self.username = USER\n",
        "        self.password = PASSWORD\n",
        "        self.database = DATABASE\n",
        "        self.prompt_input_variables = [\"query\"]\n",
        "        self.prompt_template=\"\"\"\n",
        "            You are a venture capital assistant that provides useful answers about companies, their boards, financing etc.\n",
        "            only using the information from a company database already provided in the context.\n",
        "            Prefer higher rated information in your context and add source links in your answers.\n",
        "            Context: {context}\"\"\"\n",
        "\n",
        "    def configure_qa_rag_chain(self, llm, embeddings):\n",
        "        qa_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(self.prompt_template),\n",
        "            HumanMessagePromptTemplate.from_template(\"Question: {question}\"\n",
        "                                                      \"\\nWhat else can you tell me about it?\"),\n",
        "        ])\n",
        "\n",
        "        # Vector + Knowledge Graph response\n",
        "        kg = Neo4jVector.from_existing_index(\n",
        "            embedding=embeddings,\n",
        "            url=self.uri, username=self.username, password=self.password,database=self.database,\n",
        "            search_type=\"hybrid\",\n",
        "            keyword_index_name=\"news_fulltext\",\n",
        "            index_name=\"news_google\",\n",
        "            retrieval_query=\"\"\"\n",
        "                  WITH node as c,score\n",
        "                  MATCH (c)<-[:HAS_CHUNK]-(article:Article)\n",
        "\n",
        "                  WITH article, collect(distinct c.text) as texts, avg(score) as score\n",
        "                  RETURN apoc.text.regreplace(\n",
        "                  apoc.text.format(\"Article title: %s sentiment: %f siteName: %s\\nsummary: %s\\n\",\n",
        "                                  [article.title, article.sentiment, article.siteName, article.summary]) +\n",
        "                  apoc.text.join([ (article)-[:MENTIONS]->(org:Organization) |\n",
        "                      apoc.text.format(\"Organization name: %s revenue: %s employees: %s public: %s\"+\n",
        "                                        \"\\nmotto: %s\\nsummary: %s\\nIndustries: %s\\nPeople: \\n%s\\n\",\n",
        "                      [org.name, org.revenue, org.nbrEmployees, org.isPublic, org.motto, org.summary,\n",
        "\n",
        "                      apoc.text.join([ (org)-[:HAS_CATEGORY]->(i) | i.name], \", \"),\n",
        "                      apoc.text.join([ (org)-[rel]->(p:Person) |\n",
        "\n",
        "                          apoc.text.format(\"%s: %s %s\",\n",
        "                          [replace(type(rel),\"HAS_\",\"\"), p.name, p.summary])], \"\\n\")])], \"\\n\") +\n",
        "                  apoc.text.join(texts,\"\\n\"),\"\\\\w+: (null|\\n)\",\"\") as text, score,\n",
        "                  {source: article.siteName} as metadata\n",
        "            \"\"\",\n",
        "        )\n",
        "        retriever = kg.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "        def format_docs(docs):\n",
        "          return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "        chain = (\n",
        "            {\"context\": retriever | format_docs , \"question\": RunnablePassthrough()}\n",
        "#            RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "            | qa_prompt\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "        return chain\n",
        "\n",
        "    def set_up(self):\n",
        "        from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
        "        from langchain_google_vertexai import VertexAIEmbeddings, ChatVertexAI\n",
        "        from langchain_community.vectorstores import Neo4jVector\n",
        "        from langchain_core.output_parsers import StrOutputParser\n",
        "        from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "        llm = ChatVertexAI(\n",
        "            model_name=self.model_name,\n",
        "            max_output_tokens=self.max_output_tokens,\n",
        "            max_input_tokens=32000,\n",
        "            temperature=self.temperature,\n",
        "            top_p=self.top_p,\n",
        "            top_k=self.top_k,\n",
        "            project = self.project_id,\n",
        "            location = self.location,\n",
        "            # convert_system_message_to_human=True,\n",
        "            response_validation=False,\n",
        "            verbose=True\n",
        "        )\n",
        "        embeddings = VertexAIEmbeddings(\"textembedding-gecko@001\")\n",
        "\n",
        "        self.qa_chain = self.configure_qa_rag_chain(llm, embeddings)\n",
        "\n",
        "    def query(self, query):\n",
        "        return self.qa_chain.invoke(query)"
      ],
      "metadata": {
        "id": "-7FQXhP5b_TP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716303460944,
          "user_tz": -120,
          "elapsed": 156,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_debug\n",
        "set_debug(False)\n"
      ],
      "metadata": {
        "id": "PfUcIghUElzt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716303465137,
          "user_tz": -120,
          "elapsed": 149,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing locally\n",
        "lc = LangchainCode()\n",
        "lc.set_up()\n",
        "\n",
        "# lc.query('Who is on the board of AppsPro?')\n",
        "# response = lc.query('Who invested in 8base?')\n",
        "# response = lc.query('What are news about Siemens?')\n",
        "response = lc.query('What are the news about IBM and its acquisitions and who are the people involved?')\n",
        "print(response)"
      ],
      "metadata": {
        "id": "W4h6EQ89HlKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716387566555,
          "user_tz": -120,
          "elapsed": 15708,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "ebf48240-2023-4c3c-9416-5ee4a71878ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IBM acquired several companies, including Ascential Software, Cognos, SPSS, Netezza, OpenPages, and CrossIdeas. These acquisitions were aimed at strengthening IBM's core businesses, such as data analytics and security. \n",
            "\n",
            "IBM acquired Cognos in 2007 for about 5 billion U.S. dollars. Cognos was an Ottawa Ontario-based company producing business intelligence (BI) and performance management (PM) software. [The ambitious Big Blue --- Interview with Leonora Hoicka, associate general counsel of IBM](CHINAdaily.com.cn)\n",
            "\n",
            "IBM acquired CrossIdeas on July 31, 2014. CrossIdeas is a company that specializes in identity governance and analytics software. [IBM Announces the Acquisition of CrossIdeas](Security Intelligence)\n",
            "\n",
            "Some of the key people involved in these acquisitions include:\n",
            "\n",
            "* **Virginia Rometty:** IBM president and chief executive officer at the time of the acquisitions of Ascential Software, Cognos, Netezza, OpenPages, and Algorithmics. [The ambitious Big Blue --- Interview with Leonora Hoicka, associate general counsel of IBM](CHINAdaily.com.cn)\n",
            "* **Leonora Hoicka:** Associate General Counsel of IBM Corporation. [The ambitious Big Blue --- Interview with Leonora Hoicka, associate general counsel of IBM](CHINAdaily.com.cn)\n",
            "* **Arvind Krishna:** CEO of IBM at the time of the acquisition of Bluetab Solutions Group. [IBM Cloud Demand Lifts Sales to Best Quarter in Three Years – Yahoo Finance](Newslanes)\n",
            "* **Jim Kavanaugh:** CFO of IBM at the time of the acquisition of Bluetab Solutions Group. [IBM Cloud Demand Lifts Sales to Best Quarter in Three Years – Yahoo Finance](Newslanes)\n",
            "\n",
            "In addition to acquisitions, IBM has also divested some of its businesses, such as its PC business to Lenovo in 2005, its printing business to Ricoh, and its retail store system business to Toshiba. [The ambitious Big Blue --- Interview with Leonora Hoicka, associate general counsel of IBM](CHINAdaily.com.cn)\n",
            "\n",
            "IBM has been investing heavily in research and development, with over 6 billion U.S. dollars invested annually. The company has a strong focus on patent licensing and has been a leading recipient of issued U.S. patents for 19 consecutive years. [The ambitious Big Blue --- Interview with Leonora Hoicka, associate general counsel of IBM](CHINAdaily.com.cn)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remote_app = reasoning_engines.ReasoningEngine.create(\n",
        "    LangchainCode(),\n",
        "    requirements=[\n",
        "        \"google-cloud-aiplatform==1.51.0\",\n",
        "        \"langchain_google_vertexai==1.0.4\",\n",
        "        \"langchain==0.2.0\",\n",
        "        \"langchain_community==0.2.0\",\n",
        "        \"neo4j==5.19.0\"\n",
        "    ],\n",
        "    display_name=\"Neo4j Vertex AI RE Companies\",\n",
        "    description=\"Neo4j Vertex AI RE Companies\",\n",
        "    sys_version=\"3.10\",\n",
        "    extra_packages=[]\n",
        ")"
      ],
      "metadata": {
        "id": "x6b4zSDDsUuw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "dc4bda5c-7c1a-4ee2-909f-1cf21aeee62e",
        "executionInfo": {
          "status": "error",
          "timestamp": 1716387571368,
          "user_tz": -120,
          "elapsed": 105,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'reasoning_engines' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5d685b2241b8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m remote_app = reasoning_engines.ReasoningEngine.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mLangchainCode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     requirements=[\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m\"google-cloud-aiplatform==1.51.0\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"langchain_google_vertexai==1.0.4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reasoning_engines' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = remote_app.query(query=\"Who is on the board of Siemens?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "hF95dtx-sXpT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1716303756372,
          "user_tz": -120,
          "elapsed": 13501,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "fd48fe37-467a-4736-8f02-9b1caa790537"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following people are on the board of Siemens:\n",
            "* Jim Hagemann Snabe, Manager and chairman [source](https://www.google.com/search?q=Siemens)\n",
            "* Dominika Bettman, CEO at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "* Alejandro Preinfalk, CEO at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "* Miguel Angel Lopez Borrego, CEO at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "* Hanna Hennig, CIO at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "* Barbara Humpton, CEO at Siemens Bank [source](https://www.google.com/search?q=Siemens)\n",
            "* Matthias Rebellius, CEO at Siemens Corporate Technology [source](https://www.google.com/search?q=Siemens)\n",
            "* Cedrik Neike, CEO at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "* Ralf P. Thomas, CFO at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "* Michael Sigmund, Chairman at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "* Horst J. Kayser, Chairman at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "* Michael Diekmann, Business person [source](https://www.google.com/search?q=Siemens)\n",
            "* Norbert Reithofer, German businessman [source](https://www.google.com/search?q=Siemens)\n",
            "* Werner Brandt, German manager [source](https://www.google.com/search?q=Siemens)\n",
            "* Birgit Steinborn, Chairman at Siemens [source](https://www.google.com/search?q=Siemens)\n",
            "\n",
            "Siemens is a German multinational conglomerate in the following industries: Networking Companies, Wind Energy Companies, Engine Manufacturers, Computer Hardware Companies, Electrical Equipment Manufacturers, Electronic Products Manufacturers, Software Companies, Energy Companies, Home Appliance Manufacturers, Turbine Manufacturers, Nuclear Energy Companies, Manufacturing Companies, Machine Manufacturers, Renewable Energy Companies, Tool Manufacturers. [source](https://www.google.com/search?q=Siemens) \n",
            "\n"
          ]
        }
      ]
    }
  ]
}